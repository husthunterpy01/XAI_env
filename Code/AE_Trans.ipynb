{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c3ca05fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d4fce8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import h5py\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import yaml\n",
    "import copy\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import transformers\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4bb78549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config_wbd = {}\n",
    "with open('/home/quang/Documents/XAI_env-main/Code/config.yml', 'r') as f:\n",
    "    config_wdb = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fe07d117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'bayes',\n",
       " 'metric': {'goal': 'minimize', 'name': 'test_loss_avg'},\n",
       " 'parameters': {'batch_size': {'values': [128]},\n",
       "  'd_model': {'value': 16},\n",
       "  'dff': {'values': [128, 256]},\n",
       "  'dropout': {'distribution': 'uniform', 'min': 0.05, 'max': 0.15},\n",
       "  'l_win': {'distribution': 'int_uniform', 'min': 120, 'max': 125},\n",
       "  'lr': {'distribution': 'log_uniform', 'min': -6.5, 'max': -5.5},\n",
       "  'n_epochs': {'distribution': 'int_uniform', 'min': 60, 'max': 100},\n",
       "  'n_head': {'value': 4},\n",
       "  'num_layers': {'distribution': 'int_uniform', 'min': 1, 'max': 3},\n",
       "  'weight_decay': {'distribution': 'log_uniform', 'min': -6, 'max': -4},\n",
       "  'noise_level': {'distribution': 'uniform', 'min': 0.01, 'max': 0.05},\n",
       "  'embed_dim': {'value': 16},\n",
       "  'result_dir': {'value': '/home/quang/Documents/XAI_env-main/results/'},\n",
       "  'data_dir': {'value': '/home/quang/Documents/XAI_env-main/data/processed/'}}}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_wdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5d1f5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, config, x_path, y_path):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.x_path = x_path\n",
    "        self.y_path = y_path\n",
    "        self.load_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_tensor = torch.from_numpy(self.data_[idx]).float()\n",
    "        label_tensor = torch.from_numpy(np.array(self.labels[idx])).float()\n",
    "        return {'input': input_tensor, 'labels': label_tensor}\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        x_data = {}\n",
    "        y_data = {}\n",
    "        x_path_ = os.path.join(self.config['data_dir'], self.x_path)\n",
    "        y_path_ = os.path.join(self.config['data_dir'], self.y_path)\n",
    "\n",
    "        with h5py.File(x_path_, 'r') as x_file:\n",
    "            x_data_key = list(x_file.keys())[0]\n",
    "            x_data['data'] = np.array(x_file[x_data_key])\n",
    "\n",
    "        with h5py.File(y_path_, 'r') as y_file:\n",
    "            y_data_key = list(y_file.keys())[0]\n",
    "            y_data['label'] = np.array(y_file[y_data_key])\n",
    "\n",
    "        self.data_ = x_data['data'].transpose(1,0)\n",
    "        self.data_ = np.expand_dims(self.data_, axis=0)\n",
    "        self.labels = y_data['label']\n",
    "\n",
    "        \n",
    "        \n",
    "        self.config['data_shape'] = self.data_.shape[1:]\n",
    "\n",
    "    def getshape(self):\n",
    "            return self.config['data_shape']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7c564e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim,embed_dim, noise_level):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.input_size, self.hidden_dim, self.noise_level = input_size, embed_dim,noise_level\n",
    "        self.embed_dim = embed_dim\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.input_size)\n",
    "        \n",
    "    def encoder(self,x):\n",
    "        x = self.fc1(x)\n",
    "        h1 = F.relu(x)\n",
    "        return h1\n",
    "    \n",
    "    def mask(self,x):\n",
    "        corrupted_x = x + self.noise_level + torch.randn_like(x)   # randn_like  Initializes a tensor where all the elements are sampled from a normal distribution.\n",
    "        return corrupted_x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        h2 = self.fc2(x)\n",
    "        return h2\n",
    "    \n",
    "    def forward (self, x):\n",
    "        out = self.mask(x) # Adding noise to feed the network\n",
    "        encoder = self.encoder(out)\n",
    "        decoder = self.decoder(encoder)\n",
    "        return encoder, decoder \n",
    "    \n",
    "    ## Transformer \n",
    "    ### Positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "        def __init__(self,d_model, dropout=0.0,max_len=16):\n",
    "            super(PositionalEncoding, self).__init__()\n",
    "            pe = torch.zeros(max_len,d_model)\n",
    "            position = torch.arange(0,max_len, dtype = torch.float).unsqueeze(1)\n",
    "            \n",
    "            div_term = torch.exp(torch.arange(0,d_model,2).float()*(-math.log(10000.0) / d_model))\n",
    "            \n",
    "            pe[:, 0::2] = torch.sin(position * div_term)\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "            pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "            self.register_buffer('pe', pe)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = x + self.pe[:x.size(1), :].squeeze(1)\n",
    "            return x\n",
    "        \n",
    "class Net(nn.Module):\n",
    "        def __init__(self,feature_size,num_layers,n_head,dropout,noise_level,embed_dim):\n",
    "            super(Net,self).__init__()\n",
    "            self.embed_dim = embed_dim\n",
    "            self.hidden_dim = 4*embed_dim\n",
    "            self.auto_hidden = int(feature_size / 2)\n",
    "            input_size = self.auto_hidden\n",
    "            self.pos = PositionalEncoding(d_model=input_size, max_len=input_size)\n",
    "            encoder_layers = nn.TransformerEncoderLayer(d_model=input_size, nhead=n_head, dim_feedforward=self.hidden_dim, dropout=dropout)\n",
    "            self.cell = nn.TransformerEncoder(encoder_layers,num_layers=num_layers)\n",
    "            self.linear = nn.Linear(input_size,1)\n",
    "            self.autoencoder = Autoencoder(input_size = feature_size, hidden_dim = self.auto_hidden,embed_dim = embed_dim, noise_level=noise_level)\n",
    "              \n",
    "        def forward(self,x):\n",
    "            batch_size, feature_num, feature_size = x.shape\n",
    "            encode, decode = self.autoencoder(x.view(batch_size,-1).float()) # Equals batch_size * seq_len\n",
    "            out = encode.reshape(batch_size,-1,self.auto_hidden)\n",
    "            out = self.pos(out)\n",
    "            out = out.reshape(1,batch_size,-1)  #(1,batch_size,feature_size)\n",
    "            out = self.cell(out)\n",
    "            out = out.reshape(batch_size,-1)\n",
    "            out = self.linear(out)\n",
    "            \n",
    "            return out,decode\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7027c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainer \n",
    "class ModelTrainer():\n",
    "    def __init__(self, model, train_data, criterion, optimizer, device, config):\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.train_loss_list = list()\n",
    "        self.min_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.best_optimizer = None\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        train_loss = 0.0\n",
    "        self.model.train()\n",
    "        for x, rul in self.train_data:\n",
    "            self.model.zero_grad()\n",
    "            out = self.model(x.to(self.device).float())\n",
    "            loss = torch.sqrt(self.criterion(out.float(), rul.to(self.device).float())) # RMSE\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss\n",
    "\n",
    "        train_loss = train_loss / len(self.train_data)\n",
    "        wandb.log({\"train loss\": train_loss})\n",
    "        self.train_loss_list.append(train_loss)\n",
    "\n",
    "        if train_loss < self.min_loss:\n",
    "            self.min_loss = train_loss\n",
    "            self.best_model = deepcopy(self.model.state_dict())\n",
    "            self.best_optimizer = deepcopy(self.optimizer.state_dict())\n",
    "            self.best_epoch_in_round = epoch\n",
    "    # Set up the epoch for trainning process\n",
    "    def train(self):\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        for epoch in range(1, self.config['n_epochs'] + 1):\n",
    "            self.train_epoch(epoch)\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "\n",
    "        self.config['train_loss_list'] = self.train_loss_list\n",
    "\n",
    "    def update_config(self):\n",
    "        return self.config\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a60d0545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. weight_decay uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 2. lr uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ijn5cszy\n",
      "Sweep URL: https://wandb.ai/zhukov01/RUL_Bearing/sweeps/ijn5cszy\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep=config_wdb, project='RUL_Bearing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e39b8bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "torch.manual_seed(42)\n",
    "def training():\n",
    "    with wandb.init(config = config_wbd):\n",
    "        config = wandb.config\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        train_data = CustomDataset(config, '/home/quang/Documents/XAI_env-main/data/processed/IMS/x_train.hdf5','/home/quang/Documents/XAI_env-main/data/processed/IMS/y_train.hdf5')\n",
    "        train_loader = DataLoader(train_data,\n",
    "                                  batch_size=128,\n",
    "                                  shuffle=True)\n",
    "        shape = train_data.getshape()\n",
    "        model = Net(feature_size = shape[1],\n",
    "                    noise_level = config['noise_level'],   \n",
    "                    num_layers=config['num_layers'],\n",
    "                    embed_dim = config['embed_dim'],\n",
    "                    n_head = config['n_head'],\n",
    "                    dropout=config['dropout'])\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config['weight_decay'])\n",
    "        criterion = nn.MSELoss()\n",
    "        trainer = ModelTrainer(model = model, \n",
    "                               train_data = train_loader, \n",
    "                               criterion = criterion , \n",
    "                               optimizer = optimizer, \n",
    "                               device = device, \n",
    "                               config = config)\n",
    "        trainer.train()\n",
    "\n",
    "        test_data = CustomDataset(config,'/home/quang/Documents/XAI_env-main/data/processed/IMS/x_test.hdf5','/home/quang/Documents/XAI_env-main/data/processed/IMS/y_test.hdf5')\n",
    "        test_loader = DataLoader(test_data,\n",
    "                                  batch_size=128,\n",
    "                                  shuffle=True)\n",
    "        model.to(device)\n",
    "        test_loss = 0.0\n",
    "        test_loss_list = list()\n",
    "        pred_list = list()\n",
    "        with torch.no_grad():\n",
    "            for x, rul in test_loader:\n",
    "                out = model(x.to(device).float())\n",
    "                loss = torch.sqrt(criterion(out.float(), rul.to(device).float()))\n",
    "                test_loss += loss\n",
    "                test_loss_list.append(loss)\n",
    "                pred_list.append(out.float())\n",
    "\n",
    "        test_loss_avg = test_loss / len(test_loader)\n",
    "        config['truth_list'] = truth_list\n",
    "        config['pred_list'] = pred_list\n",
    "        config['test_loss_avg'] = test_loss_avg\n",
    "        config['test_loss_list_per_id'] = test_loss_list\n",
    "        wandb.log({\"test_loss_avg\": test_loss_avg})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        val_data = CustomDataset(config,'/home/quang/Documents/XAI_env-main/data/processed/IMS/x_val.hdf5','/home/quang/Documents/XAI_env-main/data/processed/IMS/y_val.hdf5' )\n",
    "        val_loader = DataLoader(val_data,\n",
    "                                 batch_size=128,\n",
    "                                 shuffle=True)\n",
    "        model.to(device)\n",
    "        val_loss = 0.0\n",
    "        val_loss_list = list()\n",
    "        with torch.no_grad():\n",
    "            for x, rul in val_loader:\n",
    "                out = model(x.to(device).float())\n",
    "                loss = torch.sqrt(criterion(out.float(), rul.to(device).float()))\n",
    "                val_loss += loss\n",
    "                val_loss_list.append(loss)\n",
    "        val_loss_avg = val_loss / len(test_loader)\n",
    "        config['val_loss_avg'] = test_loss_avg\n",
    "        config['val_loss_list_per_id'] = test_loss_list\n",
    "        wandb.log({\"val_loss_avg\": test_loss_avg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "664f6adb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thứ hai, 12 Tháng 6 năm 2023 17:11:30 +07\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jjrv7l4a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \td_model: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_dir: /home/quang/Documents/XAI_env-main/data/processed/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdff: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.0923934647328226\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dim: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl_win: 125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0034024401507932464\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoise_level: 0.013989830032011836\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tresult_dir: /home/quang/Documents/XAI_env-main/results/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.017274217276255673\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/quang/Documents/XAI_env-main/Code/wandb/run-20230612_171133-jjrv7l4a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zhukov01/RUL_Bearing/runs/jjrv7l4a' target=\"_blank\">polished-sweep-1</a></strong> to <a href='https://wandb.ai/zhukov01/RUL_Bearing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/zhukov01/RUL_Bearing/sweeps/ijn5cszy' target=\"_blank\">https://wandb.ai/zhukov01/RUL_Bearing/sweeps/ijn5cszy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zhukov01/RUL_Bearing' target=\"_blank\">https://wandb.ai/zhukov01/RUL_Bearing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/zhukov01/RUL_Bearing/sweeps/ijn5cszy' target=\"_blank\">https://wandb.ai/zhukov01/RUL_Bearing/sweeps/ijn5cszy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zhukov01/RUL_Bearing/runs/jjrv7l4a' target=\"_blank\">https://wandb.ai/zhukov01/RUL_Bearing/runs/jjrv7l4a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114d4b4222f34435a62df1f3dd9893bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.526558…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polished-sweep-1</strong> at: <a href='https://wandb.ai/zhukov01/RUL_Bearing/runs/jjrv7l4a' target=\"_blank\">https://wandb.ai/zhukov01/RUL_Bearing/runs/jjrv7l4a</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230612_171133-jjrv7l4a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run jjrv7l4a errored: AttributeError(\"'str' object has no attribute 'to'\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run jjrv7l4a errored: AttributeError(\"'str' object has no attribute 'to'\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "wandb.agent(sweep_id, function=training ,count = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89189da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca6982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408718d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca03c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
