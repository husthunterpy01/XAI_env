{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c3ca05fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d4fce8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import h5py\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import yaml\n",
    "import copy\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import transformers\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4bb78549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config_wbd = {}\n",
    "with open('/home/quang/Documents/XAI_env-main/Code/config.yml', 'r') as f:\n",
    "    config_wdb = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fe07d117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'bayes',\n",
       " 'metric': {'goal': 'minimize', 'name': 'test_loss_avg'},\n",
       " 'parameters': {'batch_size': {'values': [128]},\n",
       "  'd_model': {'value': 16},\n",
       "  'dff': {'values': [128, 256]},\n",
       "  'dropout': {'distribution': 'uniform', 'min': 0.05, 'max': 0.15},\n",
       "  'l_win': {'distribution': 'int_uniform', 'min': 120, 'max': 125},\n",
       "  'lr': {'distribution': 'log_uniform', 'min': -6.5, 'max': -5.5},\n",
       "  'n_epochs': {'distribution': 'int_uniform', 'min': 60, 'max': 100},\n",
       "  'n_head': {'values': [1, 23]},\n",
       "  'num_layers': {'distribution': 'int_uniform', 'min': 1, 'max': 3},\n",
       "  'weight_decay': {'distribution': 'log_uniform', 'min': -6, 'max': -4},\n",
       "  'noise_level': {'distribution': 'uniform', 'min': 0.01, 'max': 0.05},\n",
       "  'embed_dim': {'value': 8},\n",
       "  'data_dir': {'value': '/home/quang/Documents/XAI_env-main/data/processed/'}}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_wdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5d1f5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "# Noisy Data loader\n",
    "# Assume that we will take the preprocessing process by the original paper\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,config,x_path,y_path):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.x_path = x_path\n",
    "        self.y_path = y_path\n",
    "        self.load_dataset()\n",
    "    # Return the length of the data\n",
    "    def __len__(self):\n",
    "        return self.data_.shape[0]\n",
    "    # Defining the dataframe and label for the data before entering the model\n",
    "    def __getitem__(self,idx):\n",
    "        return {'input':self.data_[index], \n",
    "                'labels': self.labels[index]}\n",
    "\n",
    "     # The following will be used in order to prepare to use the data_loader to put the dataset into the model\n",
    "    def load_dataset(self):\n",
    "        x_data = {}\n",
    "        y_data = {}\n",
    "        x_path_ = os.path.join(self.config['data_dir'], self.x_path)\n",
    "        y_path_ = os.path.join(self.config['data_dir'], self.y_path)\n",
    "        \n",
    "        with h5py.File(x_path_, 'r') as x_file:\n",
    "            x_data_key = list(x_file.keys())[0]  # Get the first key in the file\n",
    "            x_data['data'] = np.array(x_file[x_data_key])\n",
    "        \n",
    "        with h5py.File(y_path_, 'r') as y_file:\n",
    "            y_data_key = list(y_file.keys())[0]  # Get the first key in the file\n",
    "            y_data['label'] = np.array(y_file[y_data_key])\n",
    "            \n",
    "        self.data_ = x_data['data']\n",
    "        self.data_ = np.expand_dims(self.data_, axis=1)\n",
    "        self.labels = y_data['label']\n",
    " \n",
    "\n",
    "    # Update the shape of the data\n",
    "        self.config['data_shape'] = self.data_.shape[1:]\n",
    "    #Return the shape of the data_ everytime passing by\n",
    "    def getshape(self):\n",
    "        return self.config['data_shape']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c336bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim,embed_dim, noise_level):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.input_size, self.hidden_dim, self.noise_level = input_size, embed_dim,noise_level\n",
    "        self.embed_dim = embed_dim\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.input_size)\n",
    "        \n",
    "    def encoder(self,x):\n",
    "        x = self.fc1(x)\n",
    "        h1 = F.relu(x)\n",
    "        return h1\n",
    "    \n",
    "    def mask(self,x):\n",
    "        corrupted_x = x + self.noise_level + torch.randn_like(x)   # randn_like  Initializes a tensor where all the elements are sampled from a normal distribution.\n",
    "        return corrupted_x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        h2 = self.fc2(x)\n",
    "        return h2\n",
    "    \n",
    "    def forward (self, x):\n",
    "        out = self.mask(x) # Adding noise to feed the network\n",
    "        encoder = self.encoder(out)\n",
    "        decoder = self.decoder(encoder)\n",
    "        return encoder, decoder \n",
    "    \n",
    "    ## Transformer \n",
    "    ### Positional encoding\n",
    "    class PositionalEncoding(nn.Module):\n",
    "        def __init__(self,d_model, dropout=0.0,max_len=16):\n",
    "            pe = torch.zeros(max_len,d_model)\n",
    "            position = torch.arange(0,max_len, dtype = torch.float).unsqueeze(1)\n",
    "            \n",
    "            div_term = torch.exp(torch.arange(0,d_model,2).float()*(-math.log(10000.0) / d_model))\n",
    "            \n",
    "            pe[:, 0::2] = torch.sin(position * div_term)\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "            pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "            self.register_buffer('pe', pe)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = x + self.pe[:x.size(1), :].squeeze(1)\n",
    "            return x\n",
    "        \n",
    "    class Net(nn.Module):\n",
    "        def __init__(self,feature_size, hidden_dim,num_layers,n_head,dropout,noise_level,embed_dim):\n",
    "            super(Net,self).__init__()\n",
    "            self.embed_dim = embed_dim\n",
    "            self.hidden_dim = 4*embed_dim\n",
    "            self.auto_hidden = int(feature_size / 2)\n",
    "            input_size = self.auto_hidden\n",
    "            self.pos = PositionalEncoding(d_model=input_size, max_len=input_size)\n",
    "            encoder_layers = nn.TransformerEncoderLayer(d_model=input_size, n_head=n_head, dim_feedforward=hidden_dim, dropout=dropout)\n",
    "            self.cell = nn.TransformerEncoder(encoder_layers,num_layers=num_layers)\n",
    "            self.linear = nn.Linear(input_size,1)\n",
    "            self.autoencoder = Autoencoder(input_size = input_size, hidden_dim = self.auto_hidden, noise_level=noise_level)\n",
    "              \n",
    "        def forward(self,x):\n",
    "            batch_size, feature_num, feature_size = x.shape\n",
    "            encode, decode = self.autoencoder(x.shape(batch_size,-1)) # Equals batch_size * seq_len\n",
    "            out = encode.reshape(batch_size,-1,self.auto_hidden)\n",
    "            out = self.pos(out)\n",
    "            out = out.reshape(1,batch_size,-1)  #(1,batch_size,feature_size)\n",
    "            out = self.cell(out)\n",
    "            out = out.reshape(batch_size,-1)\n",
    "            out = self.linear(out)\n",
    "            \n",
    "            return out,decode\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7027c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainer \n",
    "class ModelTrainer():\n",
    "    def __init__(self, model, train_data, criterion, optimizer, device, config):\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.train_loss_list = list()\n",
    "        self.min_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.best_optimizer = None\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        train_loss = 0.0\n",
    "        self.model.train()\n",
    "        for x, rul in self.train_data:\n",
    "            self.model.zero_grad()\n",
    "            out = self.model(x.to(self.device).float())\n",
    "            loss = torch.sqrt(self.criterion(out.float(), rul.to(self.device).float())) # RMSE\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss\n",
    "\n",
    "        train_loss = train_loss / len(self.train_data)\n",
    "        wandb.log({\"train loss\": train_loss})\n",
    "        self.train_loss_list.append(train_loss)\n",
    "\n",
    "        if train_loss < self.min_loss:\n",
    "            self.min_loss = train_loss\n",
    "            self.best_model = deepcopy(self.model.state_dict())\n",
    "            self.best_optimizer = deepcopy(self.optimizer.state_dict())\n",
    "            self.best_epoch_in_round = epoch\n",
    "    # Set up the epoch for trainning process\n",
    "    def train(self):\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        for epoch in range(1, self.config['n_epochs'] + 1):\n",
    "            self.train_epoch(epoch)\n",
    "            wandb.log({\"epoch\": epoch})\n",
    "\n",
    "        self.config['train_loss_list'] = self.train_loss_list\n",
    "\n",
    "    def update_config(self):\n",
    "        return self.config\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a60d0545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. lr uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 2. weight_decay uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8el4h0cw\n",
      "Sweep URL: https://wandb.ai/zhukov01/RUL_Bearing/sweeps/8el4h0cw\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep=config_wdb, project='RUL_Bearing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e39b8bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "torch.manual_seed(42)\n",
    "def training():\n",
    "    with wandb.init(config = config_wbd):\n",
    "        config = wandb.config\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        train_data = CustomDataset(config, '/home/quang/Documents/XAI_env-main/data/processed/IMS/x_train.hdf5','/home/quang/Documents/XAI_env-main/data/processed/IMS/y_train.hdf5')\n",
    "        train_loader = DataLoader(train_data,\n",
    "                                  batch_size=128,\n",
    "                                  shuffle=True)\n",
    "        shape = train_data.getshape()\n",
    "        model = Net(d_model=config['d_model'],\n",
    "                    nhead=config['n_head'],\n",
    "                    feature_size = shape,\n",
    "                    noise_level = config['noise_level'],\n",
    "                    embed_dim = config['embed_dim'],\n",
    "                    n_head = configp['n_head'],\n",
    "                    num_layers=config['num_layers'],\n",
    "                    dropout=config['dropout'],\n",
    "                    l_win=config['l_win'])\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config['weight_decay'])\n",
    "        criterion = nn.MSELoss()\n",
    "        trainer = ModelTrainer(model, train_loader, criterion, optimizer, device, config)\n",
    "        trainer.train()\n",
    "\n",
    "        test_data = CustomDataset(config,'/home/quang/Documents/XAI_env-main/data/processed/IMS/x_test.hdf5','/home/quang/Documents/XAI_env-main/data/processed/IMS/y_test.hdf5')\n",
    "        test_loader = DataLoader(test_data,\n",
    "                                  batch_size=128,\n",
    "                                  shuffle=True)\n",
    "        model.to(device)\n",
    "        test_loss = 0.0\n",
    "        test_loss_list = list()\n",
    "        pred_list = list()\n",
    "        with torch.no_grad():\n",
    "            for x, rul in test_loader:\n",
    "                out = model(x.to(device).float())\n",
    "                loss = torch.sqrt(criterion(out.float(), rul.to(device).float()))\n",
    "                test_loss += loss\n",
    "                test_loss_list.append(loss)\n",
    "                pred_list.append(out.float())\n",
    "\n",
    "        test_loss_avg = test_loss / len(test_loader)\n",
    "        config['truth_list'] = truth_list\n",
    "        config['pred_list'] = pred_list\n",
    "        config['test_loss_avg'] = test_loss_avg\n",
    "        config['test_loss_list_per_id'] = test_loss_list\n",
    "        wandb.log({\"test_loss_avg\": test_loss_avg})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        val_data = CustomDataset(config,'/home/quang/Documents/XAI_env-main/data/processed/IMS/x_val.hdf5','/home/quang/Documents/XAI_env-main/data/processed/IMS/y_val.hdf5' )\n",
    "        val_loader = DataLoader(val_data,\n",
    "                                 batch_size=128,\n",
    "                                 shuffle=True)\n",
    "        model.to(device)\n",
    "        val_loss = 0.0\n",
    "        val_loss_list = list()\n",
    "        with torch.no_grad():\n",
    "            for x, rul in val_loader:\n",
    "                out = model(x.to(device).float())\n",
    "                loss = torch.sqrt(criterion(out.float(), rul.to(device).float()))\n",
    "                val_loss += loss\n",
    "                val_loss_list.append(loss)\n",
    "        val_loss_avg = val_loss / len(test_loader)\n",
    "        config['val_loss_avg'] = test_loss_avg\n",
    "        config['val_loss_list_per_id'] = test_loss_list\n",
    "        wandb.log({\"val_loss_avg\": test_loss_avg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "664f6adb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thứ tư, 31 Tháng 5 năm 2023 20:26:49 +07\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rvpi5jss with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \td_model: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_dir: /home/quang/Documents/XAI_env-main/data/processed/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdff: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1250605007022249\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dim: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl_win: 120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0018801898075674825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 82\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoise_level: 0.01570567116287472\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0031217887061847165\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/quang/Documents/XAI_env-main/Code/wandb/run-20230531_202652-rvpi5jss</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zhukov01/RUL_Bearing/runs/rvpi5jss' target=\"_blank\">lemon-sweep-1</a></strong> to <a href='https://wandb.ai/zhukov01/RUL_Bearing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/zhukov01/RUL_Bearing/sweeps/8el4h0cw' target=\"_blank\">https://wandb.ai/zhukov01/RUL_Bearing/sweeps/8el4h0cw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zhukov01/RUL_Bearing' target=\"_blank\">https://wandb.ai/zhukov01/RUL_Bearing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/zhukov01/RUL_Bearing/sweeps/8el4h0cw' target=\"_blank\">https://wandb.ai/zhukov01/RUL_Bearing/sweeps/8el4h0cw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zhukov01/RUL_Bearing/runs/rvpi5jss' target=\"_blank\">https://wandb.ai/zhukov01/RUL_Bearing/runs/rvpi5jss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3cda41c9bc41d19612ee9fb7aba5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.981159…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lemon-sweep-1</strong> at: <a href='https://wandb.ai/zhukov01/RUL_Bearing/runs/rvpi5jss' target=\"_blank\">https://wandb.ai/zhukov01/RUL_Bearing/runs/rvpi5jss</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230531_202652-rvpi5jss/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run rvpi5jss errored: NameError(\"name 'Net' is not defined\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run rvpi5jss errored: NameError(\"name 'Net' is not defined\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jyypc0vq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \td_model: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_dir: /home/quang/Documents/XAI_env-main/data/processed/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdff: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.10307458553204832\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dim: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tl_win: 125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.002019621174802421\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 87\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_head: 23\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoise_level: 0.03592094366228425\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0055082941756491146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "wandb.agent(sweep_id, function=training ,count = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89189da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca6982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408718d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca03c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
